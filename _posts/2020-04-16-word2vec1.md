---
layout: post
title:  "밑바닥에서 시작하는 딥러닝 2 <word2vec 정리(1)>"
description: 밑시딥 정리
date:   2021-04-17 
categories: [ai]
comments : true
---



이 포스팅은 밑바닥에서 시작하는 딥러닝 2를 읽으면서 개인적으로 정리한 내용들입니다. 저의 이해와 집중을 돕기 위해 작성한 포스트이므로 참고 정도로만 사용하시고 더 깊은 내용은 본 책을 구매하여 읽어보시길 추천드립니다. 

[밑바닥부터 시작하는 딥러닝 2](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=189641292)

`KEYWORD : 추론기반 기법, 신경망, 분산표현, 가중치 학습, 맥락`



### 목차

### 1. 추론기반 기법과 신경망

1. 통계기반 기법의 문제
2. 추론 기반 기법
3. 신경망에서 단어 처리

### 2. WORD2VEC

1. CBOW 구조 
2. word2vec 가중치와 분산 표현
3. CBOW 학습

<br>
<br>      

# 1. 추론기반 기법과 신경망

### (1) 통계기반 기법의 문제점

통계기반 기법 : 단어의 **발생빈도**를 활용하여 **단어의 동시발생 행렬**을 제작한 후 SVD를 사용하여 차원 축소를 한 **밀집벡터**를 얻는 방식. 

문제 : 대규모 말뭉치를 다룰 때 말뭉치의 크기에 비례하여 연산량이 늘어난다.
> SVD를 n*n 행렬에 적용하는 비용은 O(N^{3})이다. 이는 계산 시간이 n의 세제곱에 비례한다는 의미이다. 이 같이 큰 연산은 불필요할 뿐만 아니라 컴퓨팅 자원에 대한 낭비이다.

|통계기반 기법|추론 기반 기법|
|----------|-----------|
|1회의 통계 수행을 통해 **분산표현**을 얻는다.<br> 학습 데이터를 한꺼번에 처리한다. |신경망을 활용하여 **미니배치**방식으로 학습.<br>학습 데이터를 작은 단위로 나누어 학습하며 가중치를 갱신<br> **GPU를 통한 병렬 계산 가능** => 더 빠른 속도|

<br>
<br>

### (2) 추론기반 기법이란
문장의 맥락에 따라 빈 자리에 들어갈 단어를 추론하는 방식
<br>
![추론기반 기법](https://i.imgur.com/dL1r4Ou.jpg)
![추론기반 기법2](https://media.vlpt.us/post-images/dscwinterstudy/99a982e0-451e-11ea-bb1b-67735a7563ce/fig-3-3.png)
> **분포가설** 문장을 이루는 단어는 서로 영향력을 주고 받아 생성된다는 가설.
`밥을  ,  빵을  ,  사과를  +  먹었다`  의 예시처럼 목적어와 동사의 의미가 주변 단어에 의해 형성된다.

<br>
<br>
<br>

### (3) 신경망에서의 단어 처리
<br>

- **원-핫 벡터화**
    > 벡터의 원소 중 true 값은 1로 그 외의 값은 0으로 으로 처리하는 방식
    ![원핫벡터](https://media.vlpt.us/post-images/dscwinterstudy/893660d0-451a-11ea-8569-2f8cc89d4c6c/fig-3-4.png)
    >
    >- 총 어휘수 만큼의 원소를 가지는 벡터를 준비
    >- index가 단어 id와 같으면 1을, 나머지는 0으로 채워넣는다
    >- **고정 길이**를 가지는 단어벡터가 형성된다
    >- 신경망의 입력층이 뉴런의 수를 고정할 수 있게 됨
    >
    >
    >![신경망](https://media.vlpt.us/post-images/dscwinterstudy/d049b8a0-451a-11ea-8569-2f8cc89d4c6c/fig-3-5.png)
    >
    >예시 입력층 뉴런은 7개이다. <br>
    각 뉴런들은 차례대로 7개의 단어에 대응한다. 
    
    
*원-핫벡터화를 통해 단어를 신경망으로 처리할 수 있게 됐다!*
<br>
<br>

- **완전연결계층에의한 변환**

   > ![완전연결계층](https://media.vlpt.us/post-images/dscwinterstudy/36d9ac10-451b-11ea-8569-2f8cc89d4c6c/fig-3-7.png)
   >- 완전연결계층 : 각각의 노드가 이웃층의 모든 노드와 연결되어 있다. 
   >- **입력층 뉴런과 가중치의 합**이 은닉층의 뉴런이 된다
   >
   >```python
   >import numpy as np
   >
   >c = np.array([1, 0, 0. 0 ,0, 0, 0]) #입력
   >w = np.random.randn(7, 3)  #가중치
   >h = np.matmul(c, w) #중간 노드
   >print(h)
   >
   >#[[-0.70012195  -.26204755 -0.79774592]]
   >```
   >
   >**c와 w행렬의 곱** : c는 원핫벡터이므로 단어 ID에 대응하는 원소는 1이고 나머지는 0이다. *c와 w의 행렬 곱은 가중치의 행벡터 하나를 뽑아낸 것과 같다*


<br>
<br>
<br>

*word2vec의 신경망에는 CBOW와 skip-gram기법이 있다*
# word2vec : 
## 1. CBOW(Continuous Bag Of Word)
### (1) 뉴런 관점에서의 CBOW모델의 추론 처리

 ![추론처리](https://media.vlpt.us/post-images/dscwinterstudy/e66d0ee0-4518-11ea-8569-2f8cc89d4c6c/fig-3-9.png)
 > 맥락에 고려할 단어가 N개일 때(예측할 단어 근처의 맥락), 입력층의 갯수도 N개가 된다

    -  은닉층에 들어온 입력층의 값은 전체를 '평균'해서 구한다.
    - 예시 그림의 은닉층 뉴런 : (h1 + h2)/2
    - 출력층 뉴런 : 입력층 각 각 단어에 대응하는 점수이다. 값이 높을 수록 대응 단어에 대한 출현 확률도 높아진다. 
    - softmax를 적용해서 출현 가능성에 대한 '확률'을 얻는다.  

- **input이 2n개인 이유**
    ![window](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)    

    window 방식을 택하고 있기 때문이다.
    window방식은 중심 단어를 예측하기 위해 중심 단어 앞 뒤로 몇개의 단어를 확인할지 경정하는 *범위*이다. 

    ex) The fat cat (예측할 단어) on the mat
     window가 두개라면 fat, cat, on, the를 input 레이어를 넣게된다.


### (2). word2vec의 가중치와 분산표현
- **가중치(input)**
![가중치](https://media.vlpt.us/post-images/dscwinterstudy/654b15e0-4519-11ea-a244-8f351b0c9082/fig-3-10.png)    
완전연결계층의 가중치(in) W은 7x3 행렬이다.

    |행|열|
    |----------|-----------|
    |7 문장 단어 갯수<br>단어의 분산 표현<br> 학습을 진행할 수록 단어들을 더 잘 추측하는 방식으로 갱신. | 뭔지 모르겠음. 더 공부해서 포스트 갱신 예정/ 랜덤한 작은 값 |

    <br>
    
    > keypoint! `은닉층의 뉴런 수 < 입력층의 뉴런수` => 은닉층에는 필요한 정보를 간결히 담아서 밀집벡터를 얻는다. 그러기 위해 은닉층의 뉴런 수가 입력층의 뉴런 수보다 적을 필요가 있다.
    >> incoding: <br>
    >> decoding: 인코딩된 정보를 인간이 알아볼 수 있도록 복원

    > CBOW의 은닉층은 여타 신경망과 구별하기 위해 Projection layer라 부르기도 한다. 얕은 신경망은 엄밀히 따지면 딥러닝 모델은 아니다. 우리가 알고 있는 일반적인 은닉층과 다르게 활성화 함수또한 존재하지 않는다.


- **가중치 (out)**
    ![가중치out](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)
    - 투사층(은닉층)의 크기가 M이다 => CBOW수행 후 얻게될 단어 임베딩 벡터의 차원은 M이 된다
    - M X V의 가중치 행렬 를 가지게 된다
    - V는 단어 집합의 크기를 의미한다

*CBOW는 중심단어를 더 정확히 맞추기 위해 주변 단어의 행렬벡터값 가중치를 학습해가는 과정이다.*

>*학습된 in and out 가중치*
      ![학습된 in and out 가중치](https://media.vlpt.us/post-images/dscwinterstudy/f0d1a690-4516-11ea-a823-47b6cd6ac4a6/fig-3-15.png)
      출력층에는 단어의 분산 표현이 수직 방향으로 저장된다
      >> 최종적으로 활용할 단어의 가중치 : 대중적으로는 입력층의 가중치만을 최종 가중치로 선택하여 분산 표현으로 이용한다.


<BR>

### (3) 계층 관점에서 CBOW추론 처리
![계층](https://media.vlpt.us/post-images/dscwinterstudy/b563b190-4519-11ea-a244-8f351b0c9082/fig-3-11.png)


### (4) CBOW 모델 학습
**keyword** :`softmax`와 `cross entropy error`

<br>

![CBOW모델 학습](https://media.vlpt.us/post-images/dscwinterstudy/5c3fa260-4517-11ea-a244-8f351b0c9082/fig-3-12.png)

- 학습 과정
    - 올바른 예측을 위해 가중치를 조정해가는 과정
    - 가중치(in, out)에 단어의 *출현 패턴*을 파악한 벡터가 한습됨
    - 다중 클래스 분류를 수행하는 신경망에 대한 학습.
    - `softmax`를 활용해 점수를 확률로 변환
    - 확률과 정답 레이블로부터 교차 엔트로피 오차를 구함
    - 구해진 교차 엔트로피 오차를 손실로 사용해 학습을 진행한다

        [교차 엔트로피 자료](https://velog.io/@hojp7874/%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC)    
    - Softmax 계층과 Cross Entropy Error 계층은 Softmax with Loss 계층 하나로 구현할 수 있다 
