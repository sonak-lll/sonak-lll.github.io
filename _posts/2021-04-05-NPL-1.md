---
layout: post
title:  "NPL-1. 텍스트 데이터 다루기(4/5)"
description: 아이펠 노드
date:   2021-04-05 
categories: AI
permalink: /ai/
comments : true
---

going deeper session의 첫번째 노드

# NPL-1. 텍스트 데이터 다루기(4/5)

링크: https://wikidocs.net/22660
링크2: https://jjangjjong.tistory.com/41
속성: No
태그: GD - NLP

## 목표

- 분산표현에 대한 이해
- 문장 데이터 정제
- 토큰화의 기법
- 단어  Embedding 구축

# A. 전처리 : 자연어의 노이즈 제거

자연어에는 노이즈가 많다. 사람들이 일상적으로 사용하는 언어들은 교과서에 나오는 문장들처럼 띄어쓰기나 맞춤법 등이 이상적이지 않다. 변형이 될 여지가 많기 때문이다. 그렇기 때문에 소설이나 기사 같은 노이즈가 적은 데이터들을 사용하거나 이런 불순물들을 제거하기 위해 **전처리**를 수행한다.

**노이즈 유형**

- 문장부호
- 대소문자
- 특수문자

### 1. 문장부호 제거

문장 부호 양쪽에 공백을 추가하여 단어와 문장 부호를 분리해준다.

```python
def pad_punctuation(sentence, punc):
    for p in punc:
        sentence = sentence.replace(p, " " + p + " ")
#replace()함수를 사용하여 공백을 추가해준다.
    return sentence

sentence = "Hi, my name is john."

print(pad_punctuation(sentence, [".", "?", "!", ","]))
```

```python
Hi ,  my name is john .
```

### 2. 대소문자 제거

First와 first를 다른 문자로 인식하는 문제 제거 ⇒ 모든 단어를 소문자화한다.

```python
sentence = "First, open the first chapter."

print(sentence.lower())
```

```python
first, open the first chapter.
```

대문자화

```python
sentence = "First, open the first chapter."

print(sentence.upper())
```

```python
FIRST, OPEN THE FIRST CHAPTER.
```

### 3. 특수문자

```python
import re

sentence = "He is a ten-year-old boy."
sentence = re.sub("([^a-zA-Z.,?!])", " ", sentence)

print(sentence)
```

```
He is a ten year old boy.
```

**re : 정규표현식 패키지** 

### 4. 함수화

```
# From The Project Gutenberg
# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)

corpus = \
"""
In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. 
But my teacher had been with me several weeks before I understood that everything has a name.
One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. 
Some one was drawing water and my teacher placed my hand under the spout. 
As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. 
I stood still, my whole attention fixed upon the motions of her fingers. 
Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. 
I knew then that "w-a-t-e-r" meant the wonderful cool something that was flowing over my hand. 
That living word awakened my soul, gave it light, hope, joy, set it free! 
There were barriers still, it is true, but barriers that could in time be swept away.
""" 

def cleaning_text(text, punc, regex):
    # 노이즈 유형 (1) 문장부호 공백추가
    for p in punc:
        text = text.replace(p, " " + p + " ")

    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거
    text = re.sub(regex, " ", text).lower()

    return text

print(cleaning_text(corpus, [".", ",", "!", "?"], "([^a-zA-Z0-9.,?!\n])"))
```

```
in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  
but my teacher had been with me several weeks before i understood that everything has a name . 
one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  
some one was drawing water and my teacher placed my hand under the spout .  
as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  
i stood still ,  my whole attention fixed upon the motions of her fingers .  
suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  
i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  
that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  
there were barriers still ,  it is true ,  but barriers that could in time be swept away .
```

# B. 희소 표현(Sparse representation)

임베딩 레이어(Embedding Layer)를 사용하여 단어의 분산표현을 구할 수 있다.

- **희소표현 예시**

    단어를 고차원 벡터로 변환함

{
    //     [성별, 연령]
      남자: [-1.0, 0.0], // 이를테면 0.0 이 "관계없음 또는 중립적" 을 의미할 수 있겠죠!
      여자: [1.0, 0.0],
      소년: [-1.0, -0.7],
      소녀: [1.0, -0.7],
      할머니: [1.0, 0.7],
      할아버지: [-1.0, 0.7],
      아저씨: [-1.0, 0.2],
      아줌마: [1.0, 0.2]
}

```
{
//      [성별, 연령, 과일, 색깔]
      남자: [-1.0, 0.0, 0.0, 0.0],
      여자: [1.0, 0.0, 0.0, 0.0],
      사과: [0.0, 0.0, 1.0, 0.5],// 빨갛게 잘 익은 사과
      바나나: [0.0, 0.0, 1.0, -0.5]// 노랗게 잘 익은 바나나
}
```

**한계**
1. 벡터의 차원이 늘어남에 따라 0.0이 많이 발생한다. 
2. 희소 표현의 워드 벡터끼리는 단어의 유사도를 계산할 수 없다.

### 1. 코사인 유사도(Cosine Similarity)

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled.png)

두 벡터간 코사인 각도를 사용하여 벡터의 유사도를 구한다.

값이 1에 가까울 수록 유사도가 높다.

***희소 표현에 코사인 유사도를 구할 수 없는 이유***
공유하는 의미 속성이 없는 벡터의 내적은 0이다. 

# C. 분산표현(distributed representation)

**Embedding 레이어를 사용해 각 단어가 몇 차원의 속성을 가질지 정의한다**

```
embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)
```

100개의 단어를 256차원으로 표현한다. 256차원 안에는 추상적인 속성들이 분산되어 표현된다. 텍스트 데이터들을 읽어가며 적절한 표현들을 자체적으로 찾아 각각의 벡터에 배치하는 형태. 이를 통해 모델이 의미 간 유사도를 계산하거나 이를 feature삼아 모델을 학습할 수 있다.

 

# D. 토큰화

문장을 일정 기준으로 잘게 쪼갠다

1. 공백 기반 토큰화
2. 형태소 기반 토큰화

### 1. 공백 기반 토큰화

```python
corpus = \
"""
in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  
but my teacher had been with me several weeks before i understood that everything has a name . 
one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  
some one was drawing water and my teacher placed my hand under the spout .  
as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  
i stood still ,  my whole attention fixed upon the motions of her fingers .  
suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  
i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  
that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  
there were barriers still ,  it is true ,  but barriers that could in time be swept away . 
"""

tokens = corpus.split()

print("문장이 포함하는 Tokens:", tokens)
```

```
문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']
```

### 2. 형태소 기반 토큰화

```python
from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt
```

 카카오 Khaiii형태소 기반 분석기

```python
import khaiii

api = khaiii.KhaiiiApi()
api.open()
```

```python
# Khaiii를 konlpy tokenizer처럼 사용하기 위한 wrapper class입니다. 

class Khaiii():
    def pos(self, phrase, flatten=True, join=False):
        """POS tagger.

        :param flatten: If False, preserves eojeols.
        :param join: If True, returns joined sets of morph and tag.

        """
        sentences = phrase.split('\n')
        morphemes = []
        if not sentences:
            return morphemes

        for sentence in sentences:
            for word in api.analyze(sentence):
                result = [(m.lex, m.tag) for m in word.morphs]
                if join:
                    result = ['{}/{}'.format(m.lex, m.tag) for m in word.morphs]

                morphemes.append(result)

        if flatten:
            return sum(morphemes, [])

        return morphemes
```

```python
tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt(),Khaiii()]

kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'

for tokenizer in tokenizer_list:
    print('[{}] \n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))
```

# E. Wordpiece Model(WPM)

한 단어를 여러개의 subword 집합으로 본다. ex) pre+view  pre+dict

 

### 1. Byte Pair Encoding(BPE)

데이터 압축을 위해 1994년 고안됨. 

**가장 많이 등장하는 데이터 쌍을 새로운 단어로 치환하여 압축하는 방법**

aaabdaaabac # 가장 많이 등장한 바이트 쌍 "aa"를 "Z"로 치환합니다.
→ 
ZabdZabac   # "aa" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.
Z=aa        # 그다음 많이 등장한 바이트 쌍 "ab"를 "Y"로 치환합니다.
→ 
ZYdZYac     # "ab" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.
Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면
Y=ab        # 가장 많이 등장한 바이트 쌍 "ZY"를 "X"로 치환합니다.
→ 
XdXac
Z=aa
Y=ab
X=ZY       # 압축이 완료되었습니다!

**BPM을 활용하면 OOV문제를 해결할 수 있다** (Out Of Vocabulary)

```python
import re, collections

# 임의의 데이터에 포함된 단어들입니다.
# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.
vocab = {
    'l o w '      : 5,
    'l o w e r '  : 2,
    'n e w e s t ': 6,
    'w i d e s t ': 3
}

num_merges = 5

def get_stats(vocab):
    """
    단어 사전을 불러와
    단어는 공백 단위로 쪼개어 문자 list를 만들고
    빈도수와 쌍을 이루게 합니다. (symbols)
    """
    pairs = collections.defaultdict(int)
    
    for word, freq in vocab.items():
        symbols = word.split()

        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 
            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. 
        
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
        
    return v_out, pair[0] + pair[1]

token_vocab = []

for i in range(num_merges):
    print(">> Step {0}".format(i + 1))
    
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.
    vocab, merge_tok = merge_vocab(best, vocab)
    print("다음 문자 쌍을 치환:", merge_tok)
    print("변환된 Vocab:\n", vocab, "\n")
    
    token_vocab.append(merge_tok)
    
print("Merge Vocab:", token_vocab)
```

```

>> Step 1
다음 문자 쌍을 치환: es
변환된 Vocab:
 {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} 

>> Step 2
다음 문자 쌍을 치환: est
변환된 Vocab:
 {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} 

>> Step 3
다음 문자 쌍을 치환: lo
변환된 Vocab:
 {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} 

>> Step 4
다음 문자 쌍을 치환: low
변환된 Vocab:
 {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} 

>> Step 5
다음 문자 쌍을 치환: ne
변환된 Vocab:
 {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} 

Merge Vocab: ['es', 'est', 'lo', 'low', 'ne']

```

장점 :  단어의 개수를 줄임으로 메모리를 절약할 수 있다

**Embedding layer특징**
[단어의 개수 x Embedding 차원수]의 Weight를 생성

### 2. Wordpiece Model(WPM)

구글에서 BPE를 가공한 WPM 알고리즘을 제시

특이점

1. 공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가합니다.
2. 빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합칩니다.

**예시**

1. [_i, _am, _a, _b, o, y, _a, n, d, _you, _are, _a, _gir, l] 로 토큰화
2. 복원 과정의 간편함. 1) 모든 토큰을 합친 후, 2) 언더바 _를 공백으로 치환으로 마무리

[](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf)

- 조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안이 될 수 있다.
- WPM은 어떤 언어든 무관하게 적용 가능한 language-neutral하고 general한 기법이다. 한국어 형태소 분석기처럼 한국어에만 적용가능한 기법보다 훨씬 활용도가 크다.

# F .**Word2Vec**

---

[위키독스](https://wikidocs.net/22660)

단어끼리의 *연관성과* 유사도를 반영 **벡터간 단어 유사도**

### 1. 희소 표현(Sparse Representation)

벡터 또는 행렬(matrix)의 값 대부분이 0으로 표현되는 방법. 원-핫 벡터 또한 희소 벡터이다.

단점 : 단어 간 유사점을 확인 할 수 없음.

### 2. 분산 표현(Distributed Representation)

- 분포 가설(distributional hypothesis)
    - 분산 표현을 이루는 전제 조건.
    - *비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다 (맥락 의존적)*

        강아지 : 귀엽다, 예쁘다, 애교있다. 

        이들은 맥락적으로 비슷한 의미를 가진 단어이다. **분산 표현은 비슷한 단어들을 학습한 후 벡터에 단어의 의미를 여러 차원에 분산하여 표현한다.** ⇒ 저차원 벡터 생성

✔️ **Word2vec의 종류    
-** CBOW(Continuous Bag of Words) : 주변에 있는 단어로 중간 단어 예측
- Skip-Gram : 중간에 있는 단어로 주변 단어 예측

### 3. CBOW(Continuous Bag of Words)

예문 : "The fat cat sat on the mat"

{"The", "fat", "cat", "on", "the", "mat"}으로부터 sat를 예측

- 중심 단어(center word) : sat ❗예측해야 하는 단어
- 주변 단어 (context word) : ❗예측에 사용되는 단어
- 윈도우(window) : ❗중심 단어를 예측하기 위해 앞 뒤로 몇개를 볼 것인지 결정하는 *단위. 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n이 될 것입니다.*

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%201.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%201.png)

- 슬라이딩 윈도우 기법(sliding window) :  윈도우를 움직여서 주변 단어와 중심단어 선택을 바꿔가며 학습을 진행하는 기법

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%202.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%202.png)

- input layer : 윈도우 범위 안의 단어들이 원-핫 벡터로 입력

✔️ **Word2Vec은 딥 러닝 모델이 아니다!**

딥러닝은 입력층과 출력층 사이에 은닉층의 개수가 쌓여야 신경망을 학습할 수 있기 때문이다. 하지만 Word2Vec에는 입력층과 출력층 사이에 하나만의 은닉층이 존재하기 때문 ⇒ 얕은 신경망이라 부른다. 얕은 신경망에는 활성화 함수가 존재하지 않는다. 여기서 연산을 담당하는 층을 **투사층**이라 부른다.

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%203.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%203.png)

투사층의 크기 : 5 ⇒ output layer를 통과해 나온 임베딩된 단어의 차원 또한 5이다.

### 4. Skip-gram

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%204.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%204.png)

![NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%205.png](NPL-1%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%83%E1%85%A1%E1%84%85%E1%85%AE%E1%84%80%E1%85%B5(4%205)%20e030dfa61ae249f5a6c515f11adf1816/Untitled%205.png)

# G. FastText

[한국어를 위한 어휘 임베딩의 개발 -1-](https://brunch.co.kr/@learning/7)

### 1. 등장 배경 : 단어 벡터로 자연어를 처리하기에 생기는 한계점들.

- 어휘를 단어 벡터로 사용해서는 어휘간 의미적, 구문적 유사도를 평가할 수 없다
- 또한 코퍼스가 크면 자연스레 어휘의 크기도 커지기 때문에 단어 벡터도 커져 연산량이 많아진다.

### 2. fasttext의 등장(2016) : Word2Vec의 단점

- 영어가 아닌 언어에 적용하기 힘들다.
한국어는 교착어이기 때문에 동일한 단어가 다양한 문맥 속에서 다양하게 변한다.

### 3. 아이디어

어휘를 글자 수준(charater-level)로 쪼개서 학습하면 더 좋은 성능을 보인다는 2015년 이후의 연구 결과에서 기인했기 때문에 텍스트의 최소 단위를 n-gram으로 쪼갠다.
